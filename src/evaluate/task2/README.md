# Task 2: Medical QnA Evaluation System

Há»‡ thá»‘ng Ä‘Ã¡nh giÃ¡ cháº¥t lÆ°á»£ng cÃ¢u tráº£ lá»i y khoa sá»­ dá»¥ng Knowledge Graph + LLM vá»›i dataset test thá»±c táº¿.

## ğŸ“Š CÃ¡c chá»‰ sá»‘ Ä‘Ã¡nh giÃ¡ (Metrics)

### 1. **Pertinence (Pert.)** - Äá»™ phÃ¹ há»£p (15%)
- Äo lÆ°á»ng má»©c Ä‘á»™ liÃªn quan giá»¯a cÃ¢u há»i vÃ  cÃ¢u tráº£ lá»i
- Scale: 0-1 (1 = hoÃ n toÃ n phÃ¹ há»£p)
- Method: LLM-as-a-judge evaluation

### 2. **Correctness (Cor.)** - Äá»™ chÃ­nh xÃ¡c (25%) â­
- ÄÃ¡nh giÃ¡ tÃ­nh chÃ­nh xÃ¡c cá»§a ná»™i dung vá» máº·t y há»c
- Scale: 0-1 (1 = hoÃ n toÃ n chÃ­nh xÃ¡c)
- **Trá»ng sá»‘ cao nháº¥t** vÃ¬ tÃ­nh máº¡ng y khoa
- Method: LLM evaluation vá»›i KG context lÃ m ground truth

### 3. **Citation Precision (CP)** - Äá»™ chÃ­nh xÃ¡c trÃ­ch dáº«n (10%)
- Tá»· lá»‡ cÃ¡c entity Ä‘Æ°á»£c trÃ­ch dáº«n chÃ­nh xÃ¡c tá»« KG
- Scale: 0-1
- Formula: (Entities mentioned correctly) / (Total entities mentioned)
- Method: Rule-based entity matching

### 4. **Citation Recall (CR)** - Äá»™ Ä‘áº§y Ä‘á»§ trÃ­ch dáº«n (10%)
- Kháº£ nÄƒng tham chiáº¿u Ä‘áº§y Ä‘á»§ cÃ¡c entities liÃªn quan trong KG
- Scale: 0-1
- Formula: (Entities mentioned) / (Total relevant entities in KG)
- Method: Rule-based entity coverage

### 5. **Understandability (Und.)** - TÃ­nh dá»… hiá»ƒu (15%)
- Kháº£ nÄƒng diá»…n giáº£i rÃµ rÃ ng cho ngÆ°á»i khÃ´ng chuyÃªn
- Scale: 0-1 (1 = ráº¥t dá»… hiá»ƒu)
- Method: LLM evaluation for clarity

### 6. **Answer Consistency** - TÃ­nh nháº¥t quÃ¡n (10%)
- Äo tÃ­nh logic vÃ  Ä‘á»“ng nháº¥t trong cÃ¢u tráº£ lá»i
- Scale: 0-1 (1 = hoÃ n toÃ n nháº¥t quÃ¡n, khÃ´ng mÃ¢u thuáº«n)
- Method: LLM evaluation for logical consistency

### 7. **Faithfulness** - TÃ­nh trung thá»±c (15%)
- Äáº£m báº£o cÃ¢u tráº£ lá»i cÃ³ cÄƒn cá»© trong KG, khÃ´ng "hallucinate"
- Scale: 0-1 (1 = hoÃ n toÃ n dá»±a trÃªn KG)
- Method: LLM evaluation with KG grounding check

### Overall Score - Äiá»ƒm tá»•ng há»£p
Weighted average:
```
Overall = 0.15Ã—Pert + 0.25Ã—Cor + 0.10Ã—CP + 0.10Ã—CR + 0.15Ã—Und + 0.10Ã—Cons + 0.15Ã—Faith
```

## ğŸ“ Dataset

Dataset test Ä‘Æ°á»£c load tá»«:
- **Questions**: `/home/medgraph/qna/questions_en.txt` (42 cÃ¢u há»i)
- **Answers**: `/home/medgraph/qna/answers_en.txt` (42 cÃ¢u tráº£ lá»i ground truth)

Má»—i file cÃ³ 1 cÃ¢u há»i/tráº£ lá»i trÃªn má»—i dÃ²ng, tÆ°Æ¡ng á»©ng 1-1.

## ğŸš€ CÃ¡ch sá»­ dá»¥ng

### 1. Test nhanh vá»›i cÃ¢u há»i Ä‘áº§u tiÃªn

```bash
cd /home/medgraph/src/evaluate/task2
python quick_eval.py
```

### 2. ÄÃ¡nh giÃ¡ toÃ n bá»™ dataset (42 cÃ¢u há»i)

```bash
cd /home/medgraph/src/evaluate/task2
python run_batch_eval.py
```

### 3. ÄÃ¡nh giÃ¡ má»™t pháº§n dataset

```bash
# ÄÃ¡nh giÃ¡ 5 cÃ¢u há»i Ä‘áº§u tiÃªn
python run_batch_eval.py --limit 5

# ÄÃ¡nh giÃ¡ tá»« cÃ¢u thá»© 10 Ä‘áº¿n 20
python run_batch_eval.py --start 10 --limit 10

# ÄÃ¡nh giÃ¡ vá»›i custom output
python run_batch_eval.py --output results/my_evaluation.json
```

### 4. ÄÃ¡nh giÃ¡ vá»›i custom dataset

```bash
python run_batch_eval.py \
  --questions /path/to/questions.txt \
  --answers /path/to/answers.txt \
  --output results/custom_eval.json
```

### 5. Test dataset loader

```bash
cd /home/medgraph/src/evaluate/task2
python dataset_loader.py
```

## ğŸ“‚ Cáº¥u trÃºc files

```
evaluate/task2/
â”œâ”€â”€ qna_evaluator.py      # Main evaluator class vá»›i 7 metrics
â”œâ”€â”€ dataset_loader.py     # Load questions + answers tá»« txt files
â”œâ”€â”€ quick_eval.py         # Script test nhanh 1 cÃ¢u há»i
â”œâ”€â”€ run_batch_eval.py     # Script Ä‘Ã¡nh giÃ¡ batch nhiá»u cÃ¢u há»i
â””â”€â”€ README.md            # Documentation nÃ y
```

## ğŸ“Š Output Format

Káº¿t quáº£ Ä‘Æ°á»£c lÆ°u dáº¡ng JSON:

```json
{
  "dataset_info": {
    "questions_file": "/home/medgraph/qna/questions_en.txt",
    "answers_file": "/home/medgraph/qna/answers_en.txt",
    "total_questions": 42,
    "evaluated_count": 42,
    "start_index": 0
  },
  "results": [
    {
      "question": "EMPULSE Trial: In acute heart failure...",
      "answer": "Based on the knowledge graph...",
      "gid": "abc123...",
      "ground_truth": "EMPULSE Trial: Acute Kidney Injury...",
      "metrics": {
        "pertinence": 0.95,
        "correctness": 0.88,
        "citation_precision": 0.75,
        "citation_recall": 0.82,
        "understandability": 0.90,
        "answer_consistency": 0.93,
        "faithfulness": 0.85,
        "overall_score": 0.872
      },
      "kg_context_summary": {
        "entity_count": 42,
        "sample_entities": ["Heart Failure", "Empagliflozin", "Acute Kidney Injury"]
      }
    }
  ],
  "aggregate": {
    "avg_pertinence": 0.89,
    "std_pertinence": 0.05,
    "avg_correctness": 0.85,
    "std_correctness": 0.07,
    "avg_overall_score": 0.867,
    "std_overall_score": 0.04
  }
}
```

## ğŸ”„ Pipeline

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Question   â”‚ (from questions_en.txt)
â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜
       â”‚
       â”œâ”€ 1. Summarize question (process_chunks)
       â”‚
       â”œâ”€ 2. Retrieve relevant KG subgraph (seq_ret) â†’ GID
       â”‚
       â”œâ”€ 3. Extract KG context (entities + relationships)
       â”‚
       â”œâ”€ 4. Generate answer with LLM + KG context
       â”‚
       â””â”€ 5. Evaluate with 7 metrics
              â”‚
              â”œâ”€ Pertinence (LLM judge)
              â”œâ”€ Correctness (LLM + KG verification)
              â”œâ”€ Citation Precision (entity matching)
              â”œâ”€ Citation Recall (entity coverage)
              â”œâ”€ Understandability (LLM judge)
              â”œâ”€ Consistency (LLM judge)
              â””â”€ Faithfulness (KG grounding)
              
       Compare with Ground Truth (answers_en.txt)
```

## âš™ï¸ Dependencies

- **Knowledge Graph**: Neo4j vá»›i entity embeddings
- **LLM**: Gemini-2.0-flash (68 API keys vá»›i rotation)
- **Embeddings**: bge-m3 (1024-dim)
- **Evaluation**: LLM-as-a-judge + rule-based metrics
- **Dataset Loader**: dataloader.py (load_high function)

## ğŸ“ Logs

- Main log: `logs/evaluate/task2_qna.log`
- Batch log: `logs/evaluate/batch_eval.log`

## ğŸ¯ Example Results

```bash
$ python quick_eval.py

Loading MedGraph QnA dataset...
âœ… Loaded 42 question-answer pairs

================================================================================
Quick Medical QA Evaluation Test
================================================================================

ğŸ“ Question:
EMPULSE Trial: In acute heart failure with concomitant acute kidney injury (AKI), does empagliflozin worsen renal function and electrolyte balance, leading to Major Adverse Cardiovascular Events (MACE)?

ğŸ¯ Ground Truth:
EMPULSE Trial: Acute Kidney Injury (AKI) is not a contraindication, but empagliflozin should not be initiated hastily in hemodynamically unstable patients...

ğŸ’¬ Generated Answer:
Based on the EMPULSE trial data in the knowledge graph, empagliflozin does not significantly worsen renal function in acute heart failure patients with AKI...

ğŸ“Š EVALUATION RESULTS
================================================================================
  Pertinence..................................... 0.950
  Correctness.................................... 0.880
  Citation Precision............................. 0.750
  Citation Recall................................ 0.820
  Understandability.............................. 0.900
  Answer Consistency............................. 0.930
  Faithfulness................................... 0.850
--------------------------------------------------------------------------------
  Overall Score.................................. 0.872
================================================================================
```

## ğŸ”§ Configuration

Set environment variables in `.env`:
```bash
NEO4J_URI=bolt://localhost:7687
NEO4J_USERNAME=neo4j
NEO4J_PASSWORD=your_password
GEMINI_API_KEY_1=your_key_1
GEMINI_API_KEY_2=your_key_2
...
```

## ğŸ“ˆ Performance

- Average evaluation time: ~30-60s per question
- LLM calls per question: ~7-10 (depending on metrics)
- Rate limiting: Auto-managed vá»›i 68 API keys
- Memory usage: ~2-4GB (embedding model + KG queries)

## ğŸš¨ Troubleshooting

**Error: No relevant knowledge graph found**
- Check if Neo4j has data imported
- Verify KG has Middle layer with medical entities

**Error: All API keys exhausted**
- Wait 24 hours for quota reset
- Add more Gemini API keys to .env

**Error: Dataset mismatch**
- Ensure questions_en.txt and answers_en.txt have same number of lines
- Check file encoding (UTF-8)
